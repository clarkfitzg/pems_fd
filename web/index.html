<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <link rel="stylesheet" href="https://unpkg.com/leaflet@1.1.0/dist/leaflet.css" integrity="sha512-wcw6ts8Anuw10Mzh9Ytw4pylW8+NAD4ch3lqm9lzAsTxg0GFeJgoAtxuCLREZSC5lUXdVyo/7yfsqFjQ4S+aKw==" crossorigin=""/>
  <script src="https://unpkg.com/leaflet@1.1.0/dist/leaflet.js" integrity="sha512-mNqn2Wg7tSToJhvHcqfzLMU6J4mkOImSPTxVZAdo+lcPlk+GhZmYgACEe0x35K7YzW1zJ7XyJV/TT1MrdXvMcA==" crossorigin=""></script>
</head>
<body>
<h1>PEMS stations fundamental diagrams</h1>
<p>Clark Fitzgerald (Statistics), Professor Michael Zhang (Civil Engineering)</p>
<p>UC Davis</p>
<p>Nov 2017</p>
<p>This is a brief set of working notes for a project analyzing highway traffic sensor data.</p>
<p>This map shows traffic sensors colored based on the shape of their fundamental diagram. Zoom and click the points for more information.</p>
<div id="mapid" style="width: 1000px; height: 800px;"></div>
<script type="text/javascript" src="maps.js"></script>
<p>The capacity is the highest point on the fundamental diagram, the point where the left and mid lines meet. It's in units of vehicles per hour.</p>
<h2>Fundamental Diagrams</h2>
<p>The image below shows the medoid (average) shapes of the clusters. A few notes:</p>
<ul>
<li>Steeper slopes on the right hand side mean that traffic clears out faster</li>
<li>The green line is concave while the orange one is not.</li>
<li>Some regions are mostly orange, such as I 680 around San Ramon</li>
<li>Other regions are mixed, such as the Bay Bridge connecting San Francisco and Oakland</li>
</ul>
<p><img src="cluster_fd.svg" alt="Representative Fundamental Diagrams" /></p>
<h2>Clustering</h2>
<p>I used a kernel based method to cluster the stations. The kernel is based on the <a href="https://en.wikipedia.org/wiki/Dot_product#Functions">inner product between functions</a> over the interval (0, 1).</p>
<h2>Computation</h2>
<p>Computation currently consists of the following steps:</p>
<ul>
<li>Download 10 months of 30 second sensor data for the Bay Area from the <a href="http://pems.dot.ca.gov/">CalTrans Performance Measurement System</a> (PEMS) into Hadoop</li>
<li>Use <a href="http://clarkfitzg.github.io/2017/10/31/3-billion-rows-with-R/">Hive with R</a> to group data by station and fit the fundamental diagrams for each station</li>
<li>Compute the kernel matrix between all fundamental diagrams using numerical integration (just for fun, it also has an analytic form)</li>
</ul>
<p>Thu Nov 30 14:24:25 PST 2017</p>
<p>Beginning to write paper now.</p>
<h2>Computational Techniques</h2>
<p>The size and structure of the data presented a challenge; this is why we wanted to work with it.</p>
<p>We first downloaded 10 months from January 2016 - October 2016 of 30 second sensor data from the <a href="http://pems.dot.ca.gov/">CalTrans Performance Measurement System</a> (PEMS) http://pems.dot.ca.gov/ website onto a local server. I chose to only download those files from the San Francisco Bay Area (CalTrans district 3) because there is a large amount of data there, and we'm somewhat familiar with the roads.</p>
<p>Each file represents one day of observations. There are around 10 million rows and 26 columns per file that take up about 90 - 100 MB each when compressed on disk. Represented in memory as double precision floating point numbers each file will occupy about 2 GB of memory. This size becomes unwieldy with most programming languages. I processed 284 files total, which will take up 500+ GB if completely loaded into memory. This size motivated some new computational techniques.</p>
<p>Hive provided the crucial piece of infrastructure to process this data. Using schema on read with external files in Hadoop File System (HDFS) meant that all we had to do to load the data was copy the files to HDFS, so the load took less than 5 minutes.</p>
<p>We used Hive's <code>CLUSTER BY</code> to separate the data into different stations before analyzing the fundamental diagram for each station. Each station had around 800 million observations corresponding to one every 30 seconds for 10 months. We processed results in a streaming Map Reduce using the R language to express the analytic operations.</p>
<p>When we did this we weren't aware of the <code>RHive</code> package, which hasn't been actively maintained since 2015. Our computational model has less sophisticated interactive features, but is much more efficient for batch processing based on large groups, because groups are loaded in an operated on at a million elements at a time rather than line by line. An experiment showed that line by line processing would slow the program down by a factor of several hundred. Then we would be measuring run times in days rather than in minutes.</p>
<h2>Data Analysis</h2>
<p>We fit the fundamental diagram modeling vehicle flow per 30 seconds as a function of sensor occupancy. We used three different increasingly complex piecewise linear functions.</p>
<p>The first method used robust regression to fit curves on the left and right hand sides of a cutoff where occupancy = 0.2. We initially chose robust regression because of its resistance to outliers. These models included an intercept, so each station is represented by two linear models, which becomes 4 floating point numbers. Including the intercept means that the fundamental diagram doesn't necessarily pass through the points (0, 0) and (1, 0). In the areas of high density many didn't pass through (1, 0).</p>
<p>The second method fit three separate lines from points in different regions:</p>
<ul>
<li>Left line comes from fitting in (0, 0.1)</li>
<li>Center line comes from fitting in (0.2, 0.5)</li>
<li>Right line comes from fitting in (0.5, 1)</li>
</ul>
<p>We fit the lines using least squares subject to the constraints that the fundamental diagram must pass through (0, 0) and (1, 0). Enforcing this constraints makes for a more reasonable model, since we know that the fundamental diagram must satisfy this. We ignored the points in the region (0.1, 0.2) because points vary widely in this region as the traffic transitions to a congested state.</p>
<p>The last method used a nonparametric method based on binning the data based on the values of the occupancy and then computing means for the flow in each bin. We started out with a fixed minimum bin width of <code>w = 0.01</code>, which means that there will be no more than <code>1 / w = 100</code> bins in total. We chose 0.01 because it provides sufficient resolution for the fundamental diagram in areas of low density. Furthermore, we required that each bin has at least <code>k</code> observations in each bin. Some experimentation for a few different stations showed that choosing <code>k = 200</code> provided a visually smooth fundamental diagram.</p>
<p>TODO: Is there any theoretical statistical justification for this technique? It's somewhat a data summary technqiue. How much information is preserved?</p>
<p>Because there are more observations in areas of low occupancy we have more bins here. To construct the piecewise linear fundamental diagram we then simply define lines connecting each mean. This minimizes the assumptions we need to make about the fundamental diagram. Indeed, this derived data could be used for further analysis of empirical traffic flow.</p>
<p>Fitting the data produces a fundamental diagram for each station. However, we removed stations that satisfied any of the following conditions: - all observations in one bin are the same. This probably comes from a sensor error. - all observations had mean flow less than 1 vehicle per 30 seconds. If occupancy is nonzero and flow is always less than 1 then this means flow isn't being properly counted. - there are few observations in the area of high density. We experimented a bit and found a reasonable filter to be fewer than 10 bins in an area of occupancy greater than 0.2. These may be real phenomena in the data rather than sensor errors; it simply means that very little congestion (high occupancy) events happened at that station during the time of analysis.</p>
<p>All this filtering brought the number of stations down from 3722 to 1379, so about 37 percent of the data was preserved. Only about 50 percent of the stations even generate data. This filtering doesn't bias results, because ...</p>
<h2>Clustering</h2>
<p>For each of the fundamental diagrams we experimented with clustering based on the function inner products. Since the fundamental diagram is a function on [0, 1], the inner product between two different fundamental diagrams $$f$$ and $$g$$ is defined as</p>
<p>$$ \langle f, g \rangle = \int_0^1 f(x) \cdot g(x) dx. $$</p>
<p>Since we only considered piecewise linear functions this has a closed analytic form.</p>
<p>We computed these inner products between every pair of functions, producing something analagous to a covariance matrix $$X$$ of dimension 1379 x 1379. Then we scaled it into a correlation matrix $$Y$$ so that we only measure the similarity of the shapes, ignoring the magnitude:</p>
<p>$$ Y<em>{ij} = \frac{X</em>{ij}}{\sqrt{X<em>{ii} \cdot X</em>{jj}}} $$</p>
<p>The fundamental diagram is necessarily positive, so values can range between 0 and 1. Values near 1 imply that the shapes are very similar.</p>
<p>TODO: Find reference for clustering based on correlation matrix.</p>
<p>Let $$J$$ be a matrix where every entry is 1. We used the matrix $$J - Y$$ as the distance matrix to input into the 'Partitioning Around Medoids' algorithm. Inspection of the silhouette plots provided only slight evidence for clustering the fundamental diagrams into $$k = 2$$ groups. Silhouette plots for larger values of $$k$$ provided no evidence that there should be more groups.</p>
<p>Plotting actual fundamental diagrams for the $$k = 2$$ groups showed that there is really just one dominant shape of fundamental diagram. We failed to find real evidence of clusters based on this technique.</p>
<p><img src="" alt="../nonparametric/fd_2clusters.pdf" /></p>
<p>A more plausible explanation is that there's one dominant shape of fundamental diagram, and then some deviations from this.</p>
<p><img src="" alt="../nonparametric/fdtypicalunusual.pdf" /></p>
<p>The bold lines in the images come from the stations that have the highest median correlation to all other station. In this sense they are the &quot;median&quot; stations, and are the most centered in the data.</p>
</body>
</html>
